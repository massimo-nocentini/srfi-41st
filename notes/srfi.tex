
\documentclass[a4paper]{article}
\usepackage{graphicx}
\usepackage{euler}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage{heuristica}
\usepackage{beramono}
\usepackage{fancyvrb}
\usepackage{rotating}
\usepackage{minted}

\begin{document}


\title{Lazy programming in Smalltalk}
\author{Massimo Nocentini\\{\footnotesize\url{massimo.nocentini@unifi.it}}}
\date{\small Dipartimento di Statistica, Informatica, Applicazioni\\Viale Morgagni 65, 50134 Florence, Italy.\\\today}

\maketitle

\begin{abstract}
\textit{Streams, sometimes called lazy lists, are a sequential data structure containing elements
computed only on demand. A stream is either null or is a pair with a stream in its cdr. Since
elements of a stream are computed only when accessed, streams can be infinite. Once computed, the
value of a stream element is cached in case it is needed again. Streams without memoization were
first described by Peter Landin in 1965. Memoization became accepted as an essential feature of
streams about a decade later. Today, streams are the signature data type of functional
programming languages such as Haskell.}
\end{abstract}

\section{Rationale}

\color{gray}
Harold Abelson and Gerald Jay Sussman discuss streams at length, giving a strong justification for their use. The streams they provide are represented as a cons pair with a promise to return a stream in its cdr; for instance, a stream with elements the first three counting numbers is represented conceptually as 
\begin{Verbatim}[fontsize=\small]
(cons 1 (delay (cons 2 (delay (cons 3 (delay '()))))))
\end{Verbatim}
Philip Wadler, Walid Taha and David MacQueen describe such streams as odd because, regardless of their length, the parity of the number of constructors (\Verb|delay|, \Verb|cons|, \Verb|'()|) in the stream is odd.

The streams provided here differ from those of Abelson and Sussman, being represented as promises that contain a cons pair with a stream in its cdr; for instance, the stream with elements the first three counting numbers is represented conceptually as 
\begin{Verbatim}[fontsize=\small]
(delay (cons 1 (delay (cons 2 (delay (cons 3 (delay '())))))))
\end{Verbatim}
this is an even stream because the parity of the number of constructors in the stream is even.

Even streams are more complex than odd streams in both definition and usage, but they offer a strong benefit: they fix the off-by-one error of odd streams. Wadler, Taha and MacQueen show, for instance, that an expression like (stream->list 4 (stream-map / (stream-from 4 -1))) evaluates to (1/4 1/3 1/2 1) using even streams but fails with a divide-by-zero error using odd streams, because the next element in the stream, which will be 1/0, is evaluated before it is accessed. This extra bit of laziness is not just an interesting oddity; it is vitally critical in many circumstances, as will become apparent below.

When used effectively, the primary benefit of streams is improved modularity. Consider a process that takes a sequence of items, operating on each in turn. If the operation is complex, it may be useful to split it into two or more procedures in which the partially-processed sequence is an intermediate result. If that sequence is stored as a list, the entire intermediate result must reside in memory all at once; however, if the intermediate result is stored as a stream, it can be generated piecemeal, using only as much memory as required by a single item. This leads to a programming style that uses many small operators, each operating on the sequence of items as a whole, similar to a pipeline of unix commands.

In addition to improved modularity, streams permit a clear exposition of backtracking algorithms using the “stream of successes” technique, and they can be used to model generators and co-routines. The implicit memoization of streams makes them useful for building persistent data structures, and the laziness of streams permits some multi-pass algorithms to be executed in a single pass. Savvy programmers use streams to enhance their programs in countless ways.

There is an obvious space/time trade-off between lists and streams; lists take more space, but streams take more time (to see why, look at all the type conversions in the implementation of the stream primitives). Streams are appropriate when the sequence is truly infinite, when the space savings are needed, or when they offer a clearer exposition of the algorithms that operate on the sequence.
 

\section{Searching}

We interface with this service via the advanced search pane
\footnote{\url{http://academic.lexisnexis.eu/}} where it is possible to insert
the search terms and the target newspapers, \emph{Il Corriere della Sera} and
\emph{La Stampa} precisely. Our main focus regards the literals
\begin{Verbatim}
incertezza economica
\end{Verbatim}
and this document depends on results for requesting it. The search produces
lots of output therefore our procedure breaks it down in order to repeat itself
by one single year at a time, starting from the minimum date available up to
the present. Within each inner step we fetch articles in batches of 200 length
each because of restriction policies of LexisNexis; moreover, we use the
feature that selects only distinct articles upto a notion of similarity that is
not documented on the provider site. We request that articles have to be
provided in HTML format with the most data they can carry on, namely each
article should be a set of values for the following set of keywords 
(observe that some names are mistaken in Italian):
\begin{Verbatim}[fontsize=\footnotesize]
AUTORE, AZIENDA, BODY, BYLINE, CITTA, CITY, COMPANY, COMPANY-NUMBER,
CONTACT, CORREZIONE, COUNTRY, DATA, DATA-CARICO, DATA-CORREZIONE, DATELINE,
DESCRITTORI, DIDASCALIA, GEOGRAFICO, GEOGRAPHIC, GRAPHIC, HIGHLIGHT, HLEAD,
INDICE, INDUSTRIA, INDUSTRY, LANGUAGE, LEAD, LENGTH, LINGUA, LINGUAGGIO,
LN-IND, LOAD-DATE, LUNGHEZZA, MARKET, MERCATO, ORGANISATION, ORGANIZATION,
ORGANIZZAZIONE, PAESE, PAGE, PAGINNA, PERSON, PERSONA, PRODUCT, PUB-TYPE,
PUBBLICAZIONE, RUBRICA, SECTION, SOGGETTO, SOMMARIO, SOURCE, STATE, STATO,
SUBJECT, TERMINI, TERMS, TESTATA, TESTO, TICKER, TITOLO, TTESTATA, TYPE.
\end{Verbatim}

Finally, we ask for an HTML as clean as possible, deselecting bold
embellishments that highlight searched terms and to not include a title page
that summarizes the content of the whole set of articles.

\section{Scraping}

Collected files produced by the previous phase are then processed to extract
meaningful data that will form the \emph{corpus} of interest. In lack of a
well-formed schema we resort to scraping those files with the help of the
\emph{BeautifulSoup4} Python module, that allows us to easily walk the DOM tree
and to recognize the previously reported keywords looking for \verb|div| tags
with appropriate CSS style classes. All of this work is the result of a careful
understanding of the provided HTML files and should be considered a heuristic
compared to a deterministic approach.  Such scraping encodes each article as a
JSON object, for example:
\begin{Verbatim}[fontsize=\footnotesize]
{
  "order": 0, 
  "publishing-date": "1992-02-08", 
  "publisher": ["Copyright 1992 Editrice La Stampa SpA", 
                "La Stampa"], 
  "rubrica": "PRIMA PAGINA; Pg. 1", 
  "paginna": "1", 
  "lunghezza": "810 words", 
  "titolo": "IL RE IL MARCHESE E I SUDDITI", 
  "autore": "Deaglio Mario", 
  "body_preprocessed": "marchese ivrea vuole aiuto prosterni atto sottomissione dia 
                        pegno terre castelli tradotto linguaggio pseudo medioevale 
                        terre castelli quotidiano repubblica sintesi presidente cossiga 
                        dato marini risolvere difficolta olivetti linguaggio pseudo 
                        medioevale appropriato cossiga piccola politico concezione 
       		 feudale imprese fiat olivetti pirelli grandi gruppi altrettanti 
       		 [rest elided]", 
  "descrittori": "editoriale, lead story", 
  "pub-type": "Paper", 
  "linguaggio": "Italian / Italiano", 
  "company": "Olivetti, Fiat, Pirelli, Iri, Eni, EfimOlivetti, Fiat, Pirelli, Iri, Eni, Efim", 
  "subject": ["crisi", "industria", "presidente", "repubblica", "ministri", 
  	       "lavoro", "crisis", "industry", "president", "republic", "minister", 
              "workcrisi", "industria", "presidente", "repubblica", "ministri", 
              "lavoro", "crisis", "industry", "president", "republic", "minister", "work"], 
  "person": "Cossiga Francesco, Marini FrancoCossiga Francesco, Marini Franco", 
  "load-date": "1994-01-24", 
  "body": "IL Marchese d' Ivrea vuole il tuo aiuto? Che si prosterni, [rest elided]"
}
\end{Verbatim}

On one hand, our procedure is weak for two reasons: (1)~it depends on the
content provided and collects it as it is, therefore is not guaranteed that any
two objects have values for the same set of keywords, and (2)~if for some
reasons LexisNexis developers change the way data are encoded in the DOC tree
then our parser breaks unless we adhere to the change too. 

On the other hand, it proves to be robust for articles fetched so far (a little
extension to download URL references is in current development), stressed
against several searches even belonging to different domains.

The most important point of this phase is the definition of the property
\verb|body_preprocessed| which is a refinement of the property \verb|body|: it
is built (1)~by removing stopwords, punctuation symbols and unicodes bigrams,
(2)~by normalizing whitespaces and (3)~by clearing copyright words that usually
appear near the end of the article's body.

\section{Tf-Idf, LSI and LDA}

The scraping phase produces a set of JSON objects that are further processed in
order to build three models, Tf-Idf
\footnote{\url{https://radimrehurek.com/gensim/models/tfidfmodel.html}}, Latent
Semantic Indexing
\footnote{\url{https://radimrehurek.com/gensim/models/lsimodel.html}} and
Latent Dirichlet Allocation
\footnote{\url{https://radimrehurek.com/gensim/models/ldamodel.html}},
respectively -- the algorithms that build those models are implemented in the
\verb|Gensim|\footnote{\url{https://radimrehurek.com/gensim/index.html}} Python
module. 

First, the Tf-Idf model can be used to build the Figure \ref{fig:histogram};
second, $3$ ``topic'' out of $100$ found by the LSI model are
\begin{Verbatim}[fontsize=\footnotesize]
0.126*"tasso" + 0.121*"rispetto" + 0.111*"unita" + 0.109*"occupazione" + 
0.107*"aprile" + 0.097*"ue" + 0.096*"gennaio" + 0.096*"regioni" + 
0.092*"crescita" + 0.092*"marzo"

0.213*"eurostat" + 0.205*"ue" + 0.179*"marzo" + 0.150*"gennaio" + 
0.138*"febbraio" + 0.135*"aprile" + 0.129*"maggio" + 0.129*"euro" + 
0.120*"olanda" + 0.113*"spagna"

-0.240*"unita" + -0.155*"buste" + -0.150*"analisti" + 0.150*"ue" + 
-0.150*"paga" + 0.142*"eurostat" + -0.138*"agosto" + 0.129*"regioni" + 
0.122*"unione" + -0.120*"dato"
\end{Verbatim}
third, $3$ ``topic'' out of $100$ found by the LDA model are
\begin{Verbatim}[fontsize=\footnotesize]
0.021*"disoccupazione" + 0.017*"tasso" + 0.013*"regioni" + 0.009*"unione" + 
0.009*"europea" + 0.008*"medium" + 0.007*"piemonte" + 0.006*"comune" + 
0.006*"piemontese" + 0.005*"servizi"

0.012*"disoccupazione" + 0.009*"paesi" + 0.009*"rapporto" + 0.007*"ripresa" + 
0.006*"cee" + 0.006*"legge" + 0.006*"bruxelles" + 0.006*"economico" + 
0.005*"occupazione" + 0.005*"presidente"

0.012*"occupazione" + 0.009*"posti" + 0.009*"disoccupazione" + 0.006*"crisi" + 
0.006*"unita" + 0.006*"sociale" + 0.006*"tasso" + 0.006*"istat" +
0.005*"rispetto" + 0.005*"crescita"
\end{Verbatim}

All those additional data are attached to the whole serialization and are
computed with respect to the entire corpus; to deepen our study, the same
models can be computed repeatedly within different time slots or according to
arbitrary filtered sets of articles.

\section{Conclusions and Todos}

To finish our description we would like to observe that in general older
articles (published before the 00s) have longer bodies with respect to recent
ones; in addition, the more recent articles (published from the 05s) contains
just a chunk of the whole body and a url reference to the original publication,
thus one parsing step is required.

We would like to experiment additional models such as \emph{Dynamic Topic
Modeling}\footnote{\url{https://radimrehurek.com/gensim/models/ldaseqmodel.html}}
and \emph{Hierarchical Dirichlet
Process}\footnote{\url{https://radimrehurek.com/gensim/models/hdpmodel.html}}
and in general take inspiration from the \citet{rehurek_lrec}'s library.

In order to share data among the team, a Elasticsearch service has been setup.
It is a search engine based on the Lucene library that provides a distributed,
multitenant-capable full-text search engine with an HTTP web interface and
schema-free JSON documents, a natural destination for processed articles as
described in previous sections. 


\bibliographystyle{plainnat}
\bibliography{sample-base}

\newpage

\section*{Under the hood}
\label{sec:uth}

For the sake of clarity, we report the complete \verb|scraping.py| script that
implements the procedures described in this document.

%\inputminted[fontsize=\footnotesize]{python}{../src/scraping.py}


\end{document}
